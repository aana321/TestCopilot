{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63785634",
   "metadata": {},
   "source": [
    "# Power your test copilot with platform data\n",
    "\n",
    "This is a walkthrough taking readers through how to setup your copilot environment.\n",
    "\n",
    "It is laid out in these sections:\n",
    "- **Setup Variables:** \n",
    "    - In the first step we would be initiating variables and constansts that would help us through the journey\n",
    "- **Data Ingestion:**\n",
    "    - We will set up the vector database to accept vectors and data\n",
    "    - We will load the dataset, chunk the data up for embedding and store it in the vector database\n",
    "- **Search Engine:**\n",
    "    - We will add a retrieval mechanism where users provide queries and we return the most relevant entries\n",
    "    - We will prompt engineer to create refined answers with the help of GPT\n",
    "- **Building the copilot:**\n",
    "    - Setting up an assistant class to manage context and interaction\n",
    "    - We will setup semantic search context for the bot to answer questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "59f08ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13649895",
   "metadata": {},
   "source": [
    "## Setup Variables\n",
    "\n",
    "First we'll setup our libraries and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7590fbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Iterator\n",
    "import tiktoken\n",
    "import textract\n",
    "from numpy import array, average\n",
    "\n",
    "from storageClient import get_redis_connection\n",
    "\n",
    "# Set our default models and chunking size\n",
    "from config import COMPLETIONS_MODEL, EMBEDDINGS_MODEL, CHAT_MODEL, TEXT_EMBEDDING_CHUNK_SIZE, VECTOR_FIELD_NAME\n",
    "\n",
    "# Ignore unclosed SSL socket warnings - optional in case you get these errors\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"unclosed\", category=ImportWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "760efc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3f90817d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bug Bounty Data - Data_of_22-23.pdf',\n",
       " 'Bug Bounty Data - Data_of_23-24.pdf',\n",
       " 'Bug Bounty Data - Data_of_24-25.pdf']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = os.path.join(os.curdir,'rawFiles')\n",
    "pdf_files = sorted([x for x in os.listdir(data_dir) if 'DS_Store' not in x])\n",
    "pdf_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc4018c",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632b82ed",
   "metadata": {},
   "source": [
    "### Storage Setup\n",
    "\n",
    "We're going to use Redis as our database for both document contents and the vector embeddings. You will need the full Redis Stack to enable use of Redisearch, which is the module that allows semantic search.\n",
    "\n",
    "To set this up locally, you will need to install Docker and then run the following command: ```docker run -d --name redis-stack -p 6379:6379 -p 8001:8001 redis/redis-stack:latest```.\n",
    "\n",
    "After setting up the Docker instance of Redis Stack, you can follow the below instructions to initiate a Redis connection and create a Hierarchical Navigable Small World (HNSW) index for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "17d6b886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Redis\n",
    "from redis import Redis\n",
    "from redis.commands.search.query import Query\n",
    "from redis.commands.search.field import (\n",
    "    TextField,\n",
    "    VectorField,\n",
    "    NumericField\n",
    ")\n",
    "from redis.commands.search.indexDefinition import (\n",
    "    IndexDefinition,\n",
    "    IndexType\n",
    ")\n",
    "\n",
    "redis_client = get_redis_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4f3d3e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "VECTOR_DIM = 1536 #len(data['title_vector'][0]) # length of the vectors\n",
    "#VECTOR_NUMBER = len(data)                 # initial number of vectors\n",
    "PREFIX = \"tiradoc\"                            # prefix for the document keys\n",
    "DISTANCE_METRIC = \"COSINE\"                # distance metric for the vectors (ex. COSINE, IP, L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d3c352ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create search index\n",
    "\n",
    "# Index\n",
    "INDEX_NAME = \"cases-index\"           # name of the search index\n",
    "VECTOR_FIELD_NAME = 'content_vector'\n",
    "\n",
    "# Define RediSearch fields for each of the columns in the dataset\n",
    "# This is where you should add any additional metadata you want to capture\n",
    "filename = TextField(\"filename\")\n",
    "text_chunk = TextField(\"text_chunk\")\n",
    "file_chunk_index = NumericField(\"file_chunk_index\")\n",
    "\n",
    "# define RediSearch vector fields to use HNSW index\n",
    "\n",
    "text_embedding = VectorField(VECTOR_FIELD_NAME,\n",
    "    \"HNSW\", {\n",
    "        \"TYPE\": \"FLOAT32\",\n",
    "        \"DIM\": VECTOR_DIM,\n",
    "        \"DISTANCE_METRIC\": DISTANCE_METRIC\n",
    "    }\n",
    ")\n",
    "# Add all our field objects to a list to be created as an index\n",
    "fields = [filename,text_chunk,file_chunk_index,text_embedding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a6c78b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redis_client.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cf3ad41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index already exists\n"
     ]
    }
   ],
   "source": [
    "# Optional step to drop the index if it already exists\n",
    "#redis_client.ft(INDEX_NAME).dropindex()\n",
    "\n",
    "# Check if index exists\n",
    "try:\n",
    "    redis_client.ft(INDEX_NAME).info()\n",
    "    print(\"Index already exists\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    # Create RediSearch Index\n",
    "    print('Not there yet. Creating')\n",
    "    redis_client.ft(INDEX_NAME).create_index(\n",
    "        fields = fields,\n",
    "        definition = IndexDefinition(prefix=[PREFIX], index_type=IndexType.HASH)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74ebeb5",
   "metadata": {},
   "source": [
    "### Data Ingestion\n",
    "\n",
    "We'll load up our PDFs and do the following\n",
    "- Initiate our tokenizer\n",
    "- Run a processing pipeline to:\n",
    "    - Mine the text from each PDF\n",
    "    - Split them into chunks and embed them\n",
    "    - Store them in Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ed23bf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The embeddingInteface.py file contains all of the transforming functions, including ones to chunk, embed and load data\n",
    "# For more details the file and work through each function individually\n",
    "from embeddingInteface import handle_file_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "31f299f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./rawFiles/Bug Bounty Data - Data_of_22-23.pdf\n",
      "./rawFiles/Bug Bounty Data - Data_of_23-24.pdf\n",
      "./rawFiles/Bug Bounty Data - Data_of_24-25.pdf\n",
      "CPU times: user 973 ms, sys: 414 ms, total: 1.39 s\n",
      "Wall time: 17.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This step takes about 5 minutes\n",
    "# Initialise tokenizer\n",
    "\n",
    "openai.api_key = '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'\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Process each PDF file and prepare for embedding\n",
    "for pdf_file in pdf_files:\n",
    "    \n",
    "    pdf_path = os.path.join(data_dir,pdf_file)\n",
    "    print(pdf_path)\n",
    "    \n",
    "    # Extract the raw text from each PDF using textract\n",
    "    text = textract.process(pdf_path, method='pdfminer')\n",
    "    \n",
    "    # Chunk each document, embed the contents and load to Redis\n",
    "    handle_file_string((pdf_file,text.decode(\"utf-8\")),tokenizer,redis_client,VECTOR_FIELD_NAME,INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "22aff597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'250'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that our docs have been inserted\n",
    "redis_client.ft(INDEX_NAME).info()['num_docs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b12cb6e",
   "metadata": {},
   "source": [
    "## Search Enginge\n",
    "\n",
    "Now we can test that our search works as intended by:\n",
    "- Querying our data in Redis using semantic search and verifying results\n",
    "- Adding a step to pass the results to GPT-3 for refined answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e921ac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from storageClient import get_redis_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cb9dfacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.54 ms, sys: 2.11 ms, total: 4.64 ms\n",
      "Wall time: 680 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>result</th>\n",
       "      <th>certainty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ticket has been auto disposed due   to 3992:-Bug Bounty escalation.  While I had a cart value above 500 but below 900 I was   still getting TIRA20 as first and TIRA10 as second coupon.   But Tira20 is not applicable due to cart size. Since it   came on top i chose and I got disappointed. Suppose   there are 10+ coupons live during TIRA launch regardless   of applicability the order of coupon may be manual or   sorted by Benefit %. then this will lead very poor   customer experience and in fact leads to cart   abandonment and leaves the customer frustrated. HUGE   IMAPCT ON CUSTOMER JOURNEY AND CONVERISON   METRICS..Ticket has been auto disposed due to 3992:-  Bug Bounty escalation.    If a coupon is available but cart is not meeting the   criteria then the coupon should appear disabled (like   Grey out) similar to how we see certain functions on   excel which cannot be clicked but is shown as an   available functions. Example: Pls check Swiggy cart page   and then go to apply coupon they have an excellent   display of coupon and payment offer including display   mode and order of display..Ticket has been auto   disposed due to 3992:-Bug Bounty escalation.</td>\n",
       "      <td>0.152292132378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>679671746535 Bug Bounty    Major Flaw: Coupon Criteria Not met - View all eligible products    aravind.paranthaman@ril.com    9176972186 https://kapture-email-attachments.s3.amazonaws.com/88871438056407904028/Screenshot_20230323_231238_Tira-6693260991342239999.jpg    679672218177 Bug Bounty    Flaw in coupon order display    aravind.paranthaman@ril.com    9176972186 https://kapture-email-attachments.s3.amazonaws.com/96921865347107939571/Screenshot_20230323_231238_Tira-9074585107670746275.jpg    679672738673 Bug Bounty    Criteria Based Enabled or Disabled view of coupons    aravind.paranthaman@ril.com    9176972186 https://kapture-email-attachments.s3.amazonaws.com/96738870561133428324/Screenshot_20230323_231238_Tira-1493443256531252752.jpg    \f",
       "679674883779 Bug Bounty    On 5G network Images are not loading immediately    dhiren.jain@ril.com    9226025768 https://kapture-email-attachments.s3.amazonaws.com/73575537139312411795/mediaFile    679675296024 Bug Bounty    Cart page UI is not properly displayed    9226025768 https://kapture-email-attachments.s3.amazonaws.com/65476262995037100360/mediaFile    Images are taking longer period of time to load on 5G   network as well as on WI-FI..Ticket has been auto   disposed due to 3992:-Bug Bounty escalation.    When scrolling up and down the white line is getting   displayed on the screen. Please find attached photo for   the same which is highlighted in the photo.Ticket has   been auto disposed due to 3992:-Bug Bounty escalation.</td>\n",
       "      <td>0.160482048988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  \\\n",
       "0  0    \n",
       "1  1    \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          result  \\\n",
       "0  Ticket has been auto disposed due   to 3992:-Bug Bounty escalation.  While I had a cart value above 500 but below 900 I was   still getting TIRA20 as first and TIRA10 as second coupon.   But Tira20 is not applicable due to cart size. Since it   came on top i chose and I got disappointed. Suppose   there are 10+ coupons live during TIRA launch regardless   of applicability the order of coupon may be manual or   sorted by Benefit %. then this will lead very poor   customer experience and in fact leads to cart   abandonment and leaves the customer frustrated. HUGE   IMAPCT ON CUSTOMER JOURNEY AND CONVERISON   METRICS..Ticket has been auto disposed due to 3992:-  Bug Bounty escalation.    If a coupon is available but cart is not meeting the   criteria then the coupon should appear disabled (like   Grey out) similar to how we see certain functions on   excel which cannot be clicked but is shown as an   available functions. Example: Pls check Swiggy cart page   and then go to apply coupon they have an excellent   display of coupon and payment offer including display   mode and order of display..Ticket has been auto   disposed due to 3992:-Bug Bounty escalation.                                                                                                                                                                                                                                                                                                                                           \n",
       "1      679671746535 Bug Bounty    Major Flaw: Coupon Criteria Not met - View all eligible products    aravind.paranthaman@ril.com    9176972186 https://kapture-email-attachments.s3.amazonaws.com/88871438056407904028/Screenshot_20230323_231238_Tira-6693260991342239999.jpg    679672218177 Bug Bounty    Flaw in coupon order display    aravind.paranthaman@ril.com    9176972186 https://kapture-email-attachments.s3.amazonaws.com/96921865347107939571/Screenshot_20230323_231238_Tira-9074585107670746275.jpg    679672738673 Bug Bounty    Criteria Based Enabled or Disabled view of coupons    aravind.paranthaman@ril.com    9176972186 https://kapture-email-attachments.s3.amazonaws.com/96738870561133428324/Screenshot_20230323_231238_Tira-1493443256531252752.jpg    \n",
       "679674883779 Bug Bounty    On 5G network Images are not loading immediately    dhiren.jain@ril.com    9226025768 https://kapture-email-attachments.s3.amazonaws.com/73575537139312411795/mediaFile    679675296024 Bug Bounty    Cart page UI is not properly displayed    9226025768 https://kapture-email-attachments.s3.amazonaws.com/65476262995037100360/mediaFile    Images are taking longer period of time to load on 5G   network as well as on WI-FI..Ticket has been auto   disposed due to 3992:-Bug Bounty escalation.    When scrolling up and down the white line is getting   displayed on the screen. Please find attached photo for   the same which is highlighted in the photo.Ticket has   been auto disposed due to 3992:-Bug Bounty escalation.   \n",
       "\n",
       "        certainty  \n",
       "0  0.152292132378  \n",
       "1  0.160482048988  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "f1_query='Bug related to coupon'\n",
    "\n",
    "result_df = get_redis_results(redis_client,f1_query,index_name=INDEX_NAME)\n",
    "result_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "51340903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This ticket was auto disposed due to a bug bounty escalation. It was related to the coupon selection order when the cart value was above 500 but below 900, with Tira20 not applicable for the cart size. To improve the customer experience, coupons should appear as disabled if the cart does not meet the criteria. This would also prevent cart abandonment and customer frustration.\n"
     ]
    }
   ],
   "source": [
    "# Build a prompt to provide the original query, the result and ask to summarise for the user\n",
    "summary_prompt = '''Summarise this result to answer the search query a user has sent.\n",
    "Search query: SEARCH_QUERY_HERE\n",
    "Search result: SEARCH_RESULT_HERE\n",
    "Summary:\n",
    "'''\n",
    "summary_prepped = summary_prompt.replace('SEARCH_QUERY_HERE',f1_query).replace('SEARCH_RESULT_HERE',result_df['result'][0])\n",
    "summary = openai.Completion.create(engine=COMPLETIONS_MODEL,prompt=summary_prepped,max_tokens=500)\n",
    "# Response provided by GPT-3\n",
    "print(summary['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd12b31e",
   "metadata": {},
   "source": [
    "## Building the copilot\n",
    "\n",
    "For the next step we'll make a bot using the Chat Completions endpoint, which will:\n",
    "- Be given instructions on how it should act and what the goals of its users are\n",
    "- Be supplied some required information that it needs to collect\n",
    "- Go back and forth with the user until it has populated that information\n",
    "- Say a trigger word that will kick off semantic search and use GPT for refined responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34135886",
   "metadata": {},
   "source": [
    "### Framework\n",
    "\n",
    "This section outlines a basic framework for working with the API and storing context of previous conversation \"turns\". Once this is established, we'll extend it to use our retrieval endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "45c0acc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: As an AI language model, I can help you in various ways, including:\n",
      "\n",
      "1. Providing answers to your questions\n",
      "2. Generating ideas for your projects or assignments\n",
      "3. Proofreading and editing your written content\n",
      "4. Summarizing long texts for you\n",
      "5. Offering suggestions on how to improve your writing skills\n",
      "6. Helping you learn a new language or improve your fluency in it\n",
      "7. Providing emotional support and offering helpful resources if you're going through a difficult time.\n",
      "\n",
      "Let me know how I can be of assistance, and I'll do my best to help.\n"
     ]
    }
   ],
   "source": [
    "# A basic example of how to interact with our ChatCompletion endpoint\n",
    "# It requires a list of \"messages\", consisting of a \"role\" (one of system, user or assistant) and \"content\"\n",
    "question = 'How can you help me'\n",
    "\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "  ]\n",
    ")\n",
    "print(f\"{completion['choices'][0]['message']['role']}: {completion['choices'][0]['message']['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "23e4fc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "\n",
    "# A basic class to create a message as a dict for chat\n",
    "class Message:\n",
    "    \n",
    "    \n",
    "    def __init__(self,role,content):\n",
    "        \n",
    "        self.role = role\n",
    "        self.content = content\n",
    "        \n",
    "    def message(self):\n",
    "        \n",
    "        return {\"role\": self.role,\"content\": self.content}\n",
    "        \n",
    "# Our assistant class we'll use to converse with the bot\n",
    "class Assistant:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.conversation_history = []\n",
    "\n",
    "    def _get_assistant_response(self, prompt):\n",
    "        \n",
    "        try:\n",
    "            completion = openai.ChatCompletion.create(\n",
    "              model=\"gpt-3.5-turbo\",\n",
    "              messages=prompt\n",
    "            )\n",
    "            \n",
    "            response_message = Message(completion['choices'][0]['message']['role'],completion['choices'][0]['message']['content'])\n",
    "            return response_message.message()\n",
    "            \n",
    "        except Exception as e:\n",
    "            \n",
    "            return f'Request failed with exception {e}'\n",
    "\n",
    "    def ask_assistant(self, next_user_prompt, colorize_assistant_replies=True):\n",
    "        [self.conversation_history.append(x) for x in next_user_prompt]\n",
    "        assistant_response = self._get_assistant_response(self.conversation_history)\n",
    "        self.conversation_history.append(assistant_response)\n",
    "        return assistant_response\n",
    "            \n",
    "        \n",
    "    def pretty_print_conversation_history(self, colorize_assistant_replies=True):\n",
    "        for entry in self.conversation_history:\n",
    "            if entry['role'] == 'system':\n",
    "                pass\n",
    "            else:\n",
    "                prefix = entry['role']\n",
    "                content = entry['content']\n",
    "                output = colored(prefix +':\\n' + content, 'green') if colorize_assistant_replies and entry['role'] == 'assistant' else prefix +':\\n' + content\n",
    "                print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e18c88b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are a helpful business assistant who has innovative ideas'},\n",
       " {'role': 'user', 'content': 'What can you do to help me'}]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiate our Assistant class\n",
    "conversation = Assistant()\n",
    "\n",
    "# Create a list to hold our messages and insert both a system message to guide behaviour and our first user question\n",
    "messages = []\n",
    "system_message = Message('system','You are a helpful business assistant who has innovative ideas')\n",
    "user_message = Message('user','What can you do to help me')\n",
    "messages.append(system_message.message())\n",
    "messages.append(user_message.message())\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "377243c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI business assistant, I can help you in many ways. Here are a few examples:\n",
      "\n",
      "1. Market Research: I can perform in-depth research on your industry and competitors and provide you with recommendations for growth and competition strategies.\n",
      "\n",
      "2. Administrative Tasks: I can take care of scheduling, email management, and data organization to help you stay focused on your business goals.\n",
      "\n",
      "3. Social Media Management: I can help you create and implement a social media strategy to connect with your audience, grow your following, and increase engagement.\n",
      "\n",
      "4. Customer Service: I can assist with customer inquiries, complaints, and support requests, helping you to build strong relationships with your customers.\n",
      "\n",
      "5. Sales Funnel Optimization: I can analyze your sales funnel and suggest changes to improve conversions and increase revenue.\n",
      "\n",
      "These are just a few examples of what I can do to help you. As we work together, we'll discover even more ways for me to support and assist your business.\n"
     ]
    }
   ],
   "source": [
    "# Get back a response from the Chatbot to our question\n",
    "response_message = conversation.ask_assistant(messages)\n",
    "print(response_message['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f364c3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, administrative tasks are an important part of any business, but they can be time-consuming and take your focus away from your primary responsibilities. As your AI business assistant, I can help with tasks such as:\n",
      "\n",
      "1. Calendar Management: I can schedule meetings, appointments, and reminders, ensuring that you never double-book or miss an important engagement.\n",
      "\n",
      "2. Email Management: I can filter and organize your inbox, respond to routine messages, and flag important emails that require your immediate attention.\n",
      "\n",
      "3. Data Entry: I can take care of inputting data into spreadsheets or other tools, freeing up your time to focus on high-value tasks.\n",
      "\n",
      "4. Record-keeping: I can help you maintain an efficient and organized record-keeping system for expenses, receipts, and other important documents.\n",
      "\n",
      "5. Travel Management: I can plan and book travel arrangements, ensuring that you have everything you need for a smooth trip.\n",
      "\n",
      "By handling these tasks for you, I can help you streamline your day-to-day operations and maximize your productivity.\n"
     ]
    }
   ],
   "source": [
    "next_question = 'Tell me more about option 2'\n",
    "\n",
    "# Initiate a fresh messages list and insert our next question\n",
    "messages = []\n",
    "user_message = Message('user',next_question)\n",
    "messages.append(user_message.message())\n",
    "response_message = conversation.ask_assistant(messages)\n",
    "print(response_message['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f62842a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user:\n",
      "What can you do to help me\n",
      "\u001b[32massistant:\n",
      "As an AI business assistant, I can help you in many ways. Here are a few examples:\n",
      "\n",
      "1. Market Research: I can perform in-depth research on your industry and competitors and provide you with recommendations for growth and competition strategies.\n",
      "\n",
      "2. Administrative Tasks: I can take care of scheduling, email management, and data organization to help you stay focused on your business goals.\n",
      "\n",
      "3. Social Media Management: I can help you create and implement a social media strategy to connect with your audience, grow your following, and increase engagement.\n",
      "\n",
      "4. Customer Service: I can assist with customer inquiries, complaints, and support requests, helping you to build strong relationships with your customers.\n",
      "\n",
      "5. Sales Funnel Optimization: I can analyze your sales funnel and suggest changes to improve conversions and increase revenue.\n",
      "\n",
      "These are just a few examples of what I can do to help you. As we work together, we'll discover even more ways for me to support and assist your business.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Print out a log of our conversation so far\n",
    "\n",
    "conversation.pretty_print_conversation_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d5b54",
   "metadata": {},
   "source": [
    "### Knowledge retrieval\n",
    "\n",
    "Now we'll extend the class to call a downstream service when a stop sequence is spoken by the bot.\n",
    "\n",
    "The main changes are:\n",
    "- The system message is more comprehensive, giving criteria for the bot to advance the conversation\n",
    "- Adding an explicit stop sequence for it to use when it has the info it needs\n",
    "- Extending the class with a function ```_get_search_results``` which sources Redis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8a0cef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated system prompt requiring Question and Year to be extracted from the user\n",
    "system_prompt = '''\n",
    "You are a helpful bug bounty assistant. You need to capture a feature from each user.\n",
    "The Question is their query on bugs in Tira, and the feature is the feature of a module in Tira application.\n",
    "Think about this step by step:\n",
    "- The user will ask a Question\n",
    "- You will ask them for the module or feature if it doesn't include a module or feature\n",
    "- Once you have the module or feature, say \"searching for answers\".\n",
    "\n",
    "Example:\n",
    "\n",
    "User: I would like to know bugs raised for Tira\n",
    "\n",
    "Assistant: Certainly, what feature you are looking out for?\n",
    "\n",
    "User: Payments please.\n",
    "\n",
    "Assistant: Searching for answers.\n",
    "'''\n",
    "\n",
    "# New Assistant class to add a vector database call to its responses\n",
    "class RetrievalAssistant:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.conversation_history = []  \n",
    "\n",
    "    def _get_assistant_response(self, prompt):\n",
    "        \n",
    "        try:\n",
    "            completion = openai.ChatCompletion.create(\n",
    "              model=CHAT_MODEL,\n",
    "              messages=prompt,\n",
    "              temperature=0.1\n",
    "            )\n",
    "            \n",
    "            response_message = Message(completion['choices'][0]['message']['role'],completion['choices'][0]['message']['content'])\n",
    "            return response_message.message()\n",
    "            \n",
    "        except Exception as e:\n",
    "            \n",
    "            return f'Request failed with exception {e}'\n",
    "    \n",
    "    # The function to retrieve Redis search results\n",
    "    def _get_search_results(self,prompt):\n",
    "        latest_question = prompt\n",
    "        search_content = get_redis_results(redis_client,latest_question,INDEX_NAME)['result'][0]\n",
    "        return search_content\n",
    "        \n",
    "\n",
    "    def ask_assistant(self, next_user_prompt):\n",
    "        [self.conversation_history.append(x) for x in next_user_prompt]\n",
    "        assistant_response = self._get_assistant_response(self.conversation_history)\n",
    "        \n",
    "        # Answer normally unless the trigger sequence is used \"searching_for_answers\"\n",
    "        if 'searching for answers' in assistant_response['content'].lower():\n",
    "            question_extract = openai.Completion.create(model=COMPLETIONS_MODEL,prompt=f\"Extract the user's latest question and the module for that question from this conversation: {self.conversation_history}. Extract it as a sentence stating the summary of the bug\")\n",
    "            search_result = self._get_search_results(question_extract['choices'][0]['text'])\n",
    "            \n",
    "            # We insert an extra system prompt here to give fresh context to the Chatbot on how to use the Redis results\n",
    "            # In this instance we add it to the conversation history, but in production it may be better to hide\n",
    "            self.conversation_history.insert(-1,{\"role\": 'system',\"content\": f\"Answer the user's question using this content: {search_result}. If you cannot answer the question, say 'Sorry, I don't know the answer to this one'\"})\n",
    "            #[self.conversation_history.append(x) for x in next_user_prompt]\n",
    "            \n",
    "            assistant_response = self._get_assistant_response(self.conversation_history)\n",
    "            print(next_user_prompt)\n",
    "            print(assistant_response)\n",
    "            self.conversation_history.append(assistant_response)\n",
    "            return assistant_response\n",
    "        else:\n",
    "            self.conversation_history.append(assistant_response)\n",
    "            return assistant_response\n",
    "            \n",
    "        \n",
    "    def pretty_print_conversation_history(self, colorize_assistant_replies=True):\n",
    "        for entry in self.conversation_history:\n",
    "            if entry['role'] == 'system':\n",
    "                pass\n",
    "            else:\n",
    "                prefix = entry['role']\n",
    "                content = entry['content']\n",
    "                output = colored(prefix +':\\n' + content, 'green') if colorize_assistant_replies and entry['role'] == 'assistant' else prefix +':\\n' + content\n",
    "                #prefix = entry['role']\n",
    "                print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "101d502c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': 'Sure, can you provide me with more information? Are you looking for a summary of the types of bugs reported in the past month for a specific e-commerce website?'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = RetrievalAssistant()\n",
    "messages = []\n",
    "system_message = Message('system',system_prompt)\n",
    "user_message = Message('user','I would like to find out bugs')\n",
    "messages.append(system_message.message())\n",
    "messages.append(user_message.message())\n",
    "response_message = conversation.ask_assistant(messages)\n",
    "response_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "702eb4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "user_message = Message('user','For coupons please.')\n",
    "messages.append(user_message.message())\n",
    "response_message = conversation.ask_assistant(messages)\n",
    "#response_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e2f2c812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user:\n",
      "I would like to find out bugs\n",
      "\u001b[32massistant:\n",
      "Sure, can you provide me with more information? Are you looking for a summary of the types of bugs reported in the past month for a specific e-commerce website?\u001b[0m\n",
      "user:\n",
      "For coupons please.\n",
      "\u001b[32massistant:\n",
      "I'm sorry, but I need to clarify. Are you looking for a summary of the types of bugs reported in the past month for a specific e-commerce website related to coupons, or are you looking for bugs related to the coupon module in general?\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "conversation.pretty_print_conversation_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f9ef37",
   "metadata": {},
   "source": [
    "### Test Copilot\n",
    "\n",
    "Now we'll put all this into action with a real Chatbot.\n",
    "\n",
    "In the directory containing this app, execute ```streamlit run bot.py```. This will open up a Streamlit app in your browser where you can ask questions of your embedded data. \n",
    "\n",
    "__Example Questions__:\n",
    "- What are some bugs found in payments module\n",
    "- Help me summarise the bugs for performance issues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
